from src.gold_crawler import *  # CÃ¡c class API crawl
# from database.database import GoldDatabase
from datetime import datetime
import pandas as pd

# Map giá»¯a .env key vÃ  class API
apis = {
    "BTMC_DAILY": BTMCAPI,
    "SJC_DAILY": SJCAPI,
    "PNJ_DAILY": PNJAPI,
    "DOJI_DAILY": DOJIAPI,
    "PHU_QUY_DAILY": PhuQuyAPI,
}

def crawl_all_sources() -> dict:
    """
    Crawl táº¥t cáº£ sources, tráº£ vá» dict: {source_name: dataframe}
    """
    result = {}
    for env_key, api_class in apis.items():
        try:
            print(f"ğŸš€ Crawling: {env_key}")
            api_instance = api_class(env_key)
            response = api_instance.fetch_data()
            df = api_instance.transform(response)

            # Chuáº©n hÃ³a cá»™t vá» lowercase
            # print(df.columns)
            df.columns = [col.lower() for col in df.columns]

            # ThÃªm metadata
            df["source"] = env_key.lower()
            df["crawl_time"] = datetime.now().isoformat()

            result[env_key] = df

        except Exception as e:
            print(f"âŒ Lá»—i crawl {env_key}: {str(e)}")
            result[env_key] = None  # LÆ°u None Ä‘á»ƒ biáº¿t Ä‘Ã£ fail
            continue

    return result

def main():
    # db = GoldDatabase()
    crawl_results = crawl_all_sources()

    all_dfs = {}

    for env_key, df in crawl_results.items():
        if df is not None and not df.empty:
            print(f"âœ” Data tá»« {env_key} thu Ä‘Æ°á»£c {len(df)} rows.")
            # print(df)
            all_dfs[env_key] = df

            # # Insert riÃªng tá»«ng nguá»“n, source Ä‘Ã£ náº±m trong df["source"]
            # db.insert_dataframe(df, source=env_key)
        else:
            print(f"âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ tá»« {env_key} hoáº·c crawl lá»—i.")

    # db.close()

if __name__ == "__main__":
    main()
